- Starts with a random decision boundary
- Learns by adjusting weights with the perceptron trick
- Stops when all points are correctly classified or max iterations are reached
- Works only for linearly separable data (fails for XOR-type problems)
## How can we transform a line in perceptron

### Why Do We Need Transformation?

We need transformation because when training a model, we check whether new data belongs to the **positive** or **negative** region. In simple terms, we plot the data on a graph and see if it is on the correct side of the decision boundary (line).

If a data point is in the **wrong region**, we **cannot move the data itself**. Instead, we adjust the **decision boundary (line)** by updating its position, angle, or slope to correctly separate the data.

### How do we do the transformation?

1. First we check if the is in the correct region or not

if it is in the correct position we do nothing

1. But if a data which is positive but in the wrong region [negative region]

then we need to add the coefficients of x,y [2d] and c (bias) with new data’s point x,y,1

here our new data point is (2,2) and coefficients are 1,1,-5  
so we need to add (1,1,-5) with (2,2,1) to transform the boundary line  

### **How Do We Do the Transformation?**

1. **Check if the data is in the correct region.**
    - If it is in the correct region, we do nothing.
        
        ![[/image 7.png|image 7.png]]
        
2. **If a positive data point is in the wrong region (negative region):**
    
    ![[/image 1 2.png|image 1 2.png]]
    
    - We update the coefficients (**weights and bias**) by adding the data point’s values.
        
        ![[/image 2 2.png|image 2 2.png]]
        
    - Formula: **(W1, W2, b) = (W1 + x, W2 + y, b + 1)**
    
    **Example:**
    
    - Suppose the new data point is **(2,2)**, and the current coefficients are **(1,1,-5)**.
    - We update:
        
        (1+2,1+2,−5+1)=(3,3,−4)(1+2, 1+2, -5+1) = (3,3,-4)
        
        ![[/image 3 2.png|image 3 2.png]]
        
    - This moves the decision boundary in the right direction.
3. **If a negative data point is in the wrong region (positive region):**
    
    - We update the coefficients by **subtracting** the data point’s values.
    - Formula: **(W1, W2, b) = (W1 - x, W2 - y, b - 1)**
    
    **Example:**
    
    - If the new data point is **(3,1)** and the current coefficients are **(3,3,-4)**, we update:
        
        (3−3,3−1,−4−1)=(0,2,−5)(3-3, 3-1, -4-1) = (0,2,-5)
        
    - This moves the boundary in the right direction to correctly classify the data.

This process **repeats** for each misclassified point until the decision boundary correctly separates the data.

### **What About Learning Rate?**

The **learning rate (η)** controls **how much** we adjust the weights and bias when updating the decision boundary.

In the previous example we did the transformation without using learning rate.

### **With Learning Rate (η)**

- Instead of making **big jumps**, we scale the update by multiplying with a small value **η** (like 0.01 or 0.1).
- New update rules:
    - **For positive misclassified points:**
        
        $$W1=W1+η⋅x,\newline W2=W2+η⋅y, \newline b=b+η$$
        
    - **For negative misclassified points:**
        
        $$W1=W1−η⋅x,\newline W2=W2−η⋅y,\newline b=b−η \newline$$
        
- This makes the learning process **smoother** and helps the perceptron converge properly.

### **3. Choosing the Right Learning Rate**

- **Too High (η = 1.0 or more)** → The decision boundary jumps **too much** and may never settle.
- **Too Low (η = 0.0001)** → The boundary moves **too slowly** and takes forever to learn.
- **Optimal Value (η = 0.01 to 0.1)** → Smooth and efficient learning.

## Algorithm

```Python
1. Initialize weights W = [w1, w2, ..., wn] randomly
2. Initialize bias b randomly
3. Set learning rate η

4. Repeat until convergence (or max iterations): # max iteration or epoch = 1000
    For each training example (X, y):
        1. Compute z = W · X + b
        2. Predict output ŷ = sign(z)
        3. If ŷ is incorrect (ŷ ≠ y):
            - Update weights: W = W + η * (y - ŷ) * X
            - Update bias: b = b + η * (y - ŷ)
            
5. Return final weights and bias
```

### **How (y - ŷ) Determines the Update**

We calculate **(y - ŷ)** to decide whether to update the weights and bias.

|   |   |   |   |   |
|---|---|---|---|---|
|**y (Actual Class)**|**ŷ (Predicted Class)**|**y - ŷ**|**Meaning**|**Action**|
|0|0|0|Correct classification|No change|
|1|1|0|Correct classification|No change|
|1|0|1|**Positive point in negative region**|**Add** (increase weights & bias)|
|0|1|-1|**Negative point in positive region**|**Subtract** (decrease weights & bias)|

### **Calculation Example (2D case)**

### **Given:**

- **Initial weights**: $( W_1 = 1, W_2 = 1 )$
- **Initial bias**: $( b = -5 )$
- **Learning rate** :$( eta = 0.1 )$
- **New data point**: $( (x_1, x_2) = (2,2) )$
- **Actual class (y)**: 1

### **Step 1: Compute Prediction**

$z = W_1 \cdot x_1 + W_2 \cdot x_2 + b$  
  
$z = (1 \cdot 2) + (1 \cdot 2) + (-5) = 2 + 2 - 5 = -1$

  
$Since ( z < 0 ), the perceptron predicts ŷ = 0.$

### **Step 2: Compute Update**

  
$y - ŷ = 1 - 0 = 1$  
  
Since $( y - ŷ = 1 )$, the point is **positive but in the negative region**, so we **add** the update.

### **Step 3: Update Weights and Bias**

  
$W_1 = W_1 + \eta \cdot x_1 = 1 + (0.1 \cdot 2) = 1.2$  
  
$W_2 = W_2 + \eta \cdot x_2 = 1 + (0.1 \cdot 2) = 1.2$  
  
$b = b + \eta \cdot 1 = -5 + 0.1 = -4.9$

### **Updated Values:**

- $( W_1 = 1.2 )$
- $( W_2 = 1.2 )$
- $( b = -4.9 )$