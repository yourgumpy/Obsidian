![[/image 8.png|image 8.png]]
Summary of the previous two topic


![[CSE465_Online_class_1.pdf]]

Ok, there are some fundamental flaws in perception like;

### **Flaws of the Perceptron Algorithm**

1️⃣ Only works for linearly separable data (fails on XOR-like problems).

2️⃣ No probability or confidence score (outputs only 0 or 1).

3️⃣ Gets stuck in infinite loops if data is not separable.

4️⃣ Fixed learning rate (too high = unstable, too low = slow).

5️⃣ Sensitive to noise (fails with overlapping classes).

6️⃣ Only supports binary classification (cannot handle multi-class problems).

## Loss Function

→ A **loss function** measures how far the model's prediction is from the actual value. It helps the model adjust its weights to improve accuracy.

### **Loss Function in Terms of Perceptron**

In a perceptron, we compute **error** using a function like **f(w₁, w₂, b)**.

To improve model performance, we **minimize the loss**, ensuring better predictions.

We calculate the **loss function** for misclassified points using a **dot product** approach. This involves substituting the misclassified points into the **line equation** and adjusting weights accordingly.

### **Example:**

Given the line equation:

![[/image 1 3.png|image 1 3.png]]

$$f(x, y) = 2x + 3y + 4$$

Misclassified points:

- **(-2,2)** → Should be in the **negative** region.
- **(0,2)** → Should be in the **positive** region.

### **Loss Calculation:**

For misclassified point **(-2,2)**:

$$L1 = 2(−2)+3(2)+4 = −4+6+4=6$$

Since this point should be **negative**, but the result is **positive**, we need to **subtract** it from the total loss.

For misclassified point **(0,2)**:

$$L2=2(0)+3(2)+4=0+6+4=10$$

Since this point should be **positive**, but the result is **negative**, we need to **add** it to the total loss.

### **Final Loss Function:**

$$L=−(2(−2)+3(2)+4)+(2(0)+3(2)+4)$$

$$L=−6+10=4$$

To **minimize the loss**, we update the weights using:

$$w = w + \eta \cdot y \cdot x$$

where **η** is the learning rate, **y** is the actual label, and **x** is the feature vector.

This process **iterates until the loss reaches zero** or converges.

## Loss Function from SGD

Here is the loss function for

$$L(w_1,w_2,b) = \frac{1}{n}\sum_{i=1}^{n} max(0, -y_if(x_i)) + {\alpha R(w)}$$

we can ignore the $\alpha R(w)$ as this refers to regularization. we assume we do not regularizing.

So our function can be written as

$$Loss function, L(w_1,w_2,b) = \frac{1}{n}\sum_{i=1}^{n} max(0, -y_if(x_i))$$

here;

- n = # of row in data
- $f(x_i) = w_1 + w_2 + b$
- L depend on $w_1, w_2, b$

## Example: how this loss function works

### **Given Loss Function:**

$$L(w_1, w_2, b) = \frac{1}{n} \sum_{i=1}^{n} \max(0, -y_i f(x_i))$$

where:

- $f(x) = w_1 \cdot \text{CGPA} + w_2 \cdot \text{IQ} + b$ (our decision boundary)
- $y_i$ is the actual label (**+1 for placed, -1 for not placed**)
- If is negative, the point is misclassified, and we add its loss.
    
    $y_i f(x_i)$
    
- If is positive, the loss is **0** (correct classification).
    
    $y_i f(x_i)$
    

---

### **Dataset Example (CGPA, IQ, Placement):**

|   |   |   |   |
|---|---|---|---|
|**Student**|**CGPA**|**IQ**|**Placement (y)**|
|A|3.8|120|+1 (Placed)|
|B|2.5|90|-1 (Not Placed)|
|C|3.2|110|+1 (Placed)|
|D|2.8|100|-1 (Not Placed)|

### **Assume Initial Weights and Bias:**

- $w_1$ = 0.5 (Weight for CGPA)
- $w_2$ = 0.3 (Weight for IQ)
- b = -1.0 (Bias)

---

### **Step-by-Step Calculation of f(x):**

$$f(x) = (0.5 \times \text{CGPA}) + (0.3 \times \text{IQ}) + (-1)$$

|**Student**|**f(x)** Calculation|
|---|---|
|A|$(0.5 \times 3.8) + (0.3 \times 120) - 1 = 1.9 + 36 - 1 = 36.9$|
|B|$(0.5 \times 2.5) + (0.3 \times 90) - 1 = 1.25 + 27 - 1 = 27.25$|
|C|$(0.5 \times 3.2) + (0.3 \times 110) - 1 = 1.6 + 33 - 1 = 33.6$|
|D|$(0.5 \times 2.8) + (0.3 \times 100) - 1 = 1.4 + 30 - 1 = 30.4$|

---

### **Step-by-Step Loss Calculation:**

$$L = \frac{1}{n} \sum \max(0, -y_i f(x_i))$$

|   |   |   |   |   |
|---|---|---|---|---|
|**Student**|$y_i$|$f(x_i)$|$-y_i f(x_i)$|**Loss Contribution**|
|A|+1|36.9|-36.9|**0** (Correct)|
|B|-1|27.25|27.25|**27.25** (Misclassified)|
|C|+1|33.6|-33.6|**0** (Correct)|
|D|-1|30.4|30.4|**30.4** (Misclassified)|

### **Final Loss Calculation:**

$L = \frac{1}{4} \left( 0 + 27.25 + 0 + 30.4 \right)$

$L = \frac{57.65}{4} = 14.41$

---

## Minimizing Loss in Perceptron Using Gradient Descent, Epochs & Partial Derivatives

Here

$$L = \frac{1}{n} \sum {erros}$$
$$L = \frac{1}{n} \sum \max(0, -y_i f(x_i))$$
where $f(x_i) = w_1x_{i1} + w_2x_{i2} + b$

### The update rules of Gradient Descent

![[/image 2 3.png|image 2 3.png]]

  
To update weights using gradient descent, we compute the **partial derivatives**:

![[/image 3 3.png|image 3 3.png]]

![[/image 4 2.png|image 4 2.png]]

## More perceptron Loss function

### **Summary of the Image Content**

1. **What is Perceptron?**
    - A perceptron is a **mathematical function** used in machine learning for binary classification.
    - It uses a **step function** as its activation function, outputting either **+1 or -1** based on the input.
2. **When and How is it Equal to Logistic Regression? → we want probabilities**
    - The perceptron can be modified for **probabilistic classification** by replacing the step function with the **sigmoid activation function**.
    - This modification leads to **logistic regression**, which predicts probabilities instead of hard class labels.
    - Logistic regression uses the **binary cross-entropy loss function**:
        
        $$L = -y_i \log \hat{y_i} + (1 - y_i) \log (1 - \hat{y_i})$$
        
3. **How it Works on Multiclass Classification?**
    - For more than two classes, perceptron use the **softmax activation function**:
        
        $$f = \frac{e^{z_i}}{\sum e^{z_j}}$$
        
    - This converts raw scores into probabilities for multiple categories.
    - The loss function used here is **categorical cross-entropy**:
        
        $$L = \sum_{j=1}^{M} y_j \log \hat{y_j}$$
        
4. **When and How it Works Like Linear Regression?**
    - If no activation function is applied, the perceptron behaves like a **linear model** where the output is simply the weighted sum of inputs:
        
        $$z \to z$$
        
    - This is equivalent to **linear regression**, where the loss function is **Mean Squared Error (MSE)**.

### Summary

|**Loss Function**|**Activation**|**Output**|**Type of Classification/Regression**|
|---|---|---|---|
|**Hinge Loss**|Step|Perceptron → {-1, 1}|Binary Classification|
|**Log-Loss (Binary Cross-Entropy)**|Sigmoid|Logistic Regression → {0, 1}|Binary Classification|
|**Categorical Cross-Entropy**|Softmax|Softmax Regression|Multiclass Classification|
|**Mean Squared Error (MSE)**|None (Identity) / Linear|Linear Regression → Continuous Output|Regression|